---
sidebar: sidebar 
permalink: nvme_esxi_7.html 
keywords: nvme, esxi, ontap, nvme/fc, hypervisor 
summary: Beschreibt die Konfiguration von NVMe-of für ESXi 7.x mit ONTAP 
---
= NVMe-of Host-Konfiguration für ESXi 7.x mit ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toc: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/
:toc-position: content




== Instandhaltung

* NVMe over Fibre Channel (NVMe/FC) wird auf ONTAP 9.7 oder höher unterstützt.
* Ab 7.0U3c und höher wird die NVMe/TCP Funktion für ESXi Hypervisor unterstützt.
* Ab ONTAP 9.10.1 und höher wird die NVMe/TCP Funktion für ONTAP unterstützt.




== Funktionen

* Auf dem ESXi-Initiator-Host kann NVMe/FC- und FCP-Datenverkehr über dieselben Adapter-Ports ausgeführt werden. Siehe link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] Für eine Liste der unterstützten FC-Adapter und Controller. Siehe link:https://mysupport.netapp.com/matrix/["NetApp Interoperabilitätsmatrix"^] Erhalten Sie auf der aktuellen Liste der unterstützten Konfigurationen und Versionen.
* Ab ONTAP 9.9.1 P3 wird die NVMe/FC-Funktion für ESXi 7.0 Update 3 unterstützt.
* Für ESXi 7.0 und neuere Versionen ist HPP (High Performance Plugin) das Standard-Plug-in für NVMe-Geräte.




== Bekannte Einschränkungen

Die folgenden Konfigurationen werden nicht unterstützt:

* RDM-Zuordnung
* VVols




== NVMe/FC aktivieren

. Überprüfen Sie den NQN-String des ESXi-Hosts und vergewissern Sie sich, dass er mit der NQN-Zeichenfolge des Hosts für das entsprechende Subsystem im ONTAP-Array übereinstimmt:
+
[listing]
----
# esxcli nvme  info get
Host NQN: nqn.2014-08.com.vmware:nvme:nvme-esx

# vserver nvme subsystem host show -vserver vserver_nvme
  Vserver Subsystem             Host NQN
  ------- ------------------- ----------------------------------------
  vserver_nvme ss_vserver_nvme nqn.2014-08.com.vmware:nvme:nvme-esx
----




=== Konfigurieren Sie Broadcom/Emulex

. Überprüfen Sie, ob die Konfiguration mit dem erforderlichen Treiber/der erforderlichen Firmware unterstützt wird, indem Sie auf verweisen link:https://mysupport.netapp.com/matrix/["NetApp Interoperabilitätsmatrix"^].
. Legen Sie den Parameter lpfc-Treiber fest `lpfc_enable_fc4_type=3` NVMe/FC-Unterstützung in der ermöglichen `lpfc` Treiber und starten Sie den Host neu.



NOTE: Ab vSphere 7.0 Update 3 werden die `brcmnvmefc` Treiber ist nicht mehr verfügbar. Deshalb, das `lpfc` Er enthält nun die bereits mit dem bereitgestellten NVMe-over-Fibre-Channel-Funktionen (NVMe/FC) `brcmnvmefc` Treiber.


NOTE: Der `lpfc_enable_fc4_type=3` Der Parameter ist standardmäßig für die Adapter der LPe35000-Serie eingestellt. Sie müssen den folgenden Befehl ausführen, um ihn manuell für Adapter der LPe32000-Serie und der LPe31000-Serie einzustellen.

[listing]
----
# esxcli system module parameters set -m lpfc -p lpfc_enable_fc4_type=3

#esxcli system module parameters list  -m lpfc | grep lpfc_enable_fc4_type
lpfc_enable_fc4_type              int     3      Defines what FC4 types are supported

#esxcli storage core adapter list
HBA Name  Driver   Link State  UID                                   Capabilities         Description
--------  -------  ----------  ------------------------------------  -------------------  -----------
vmhba1    lpfc     link-up     fc.200000109b95456f:100000109b95456f  Second Level Lun ID  (0000:86:00.0) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter    FC HBA
vmhba2    lpfc     link-up     fc.200000109b954570:100000109b954570  Second Level Lun ID  (0000:86:00.1) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter    FC HBA
vmhba64   lpfc     link-up     fc.200000109b95456f:100000109b95456f                       (0000:86:00.0) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter   NVMe HBA
vmhba65   lpfc     link-up     fc.200000109b954570:100000109b954570                       (0000:86:00.1) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter   NVMe HBA
----


=== Konfiguration von Marvell/QLogic

.Schritte
. Überprüfen Sie, ob die Konfiguration mit dem erforderlichen Treiber/der erforderlichen Firmware unterstützt wird, indem Sie auf lesen link:https://mysupport.netapp.com/matrix/["NetApp Interoperabilitätsmatrix"^].
. Stellen Sie die ein `qlnativefc` Treiberparameter `ql2xnvmesupport=1` NVMe/FC-Unterstützung in der ermöglichen `qlnativefc` Treiber und starten Sie den Host neu.
+
`# esxcfg-module -s 'ql2xnvmesupport=1' qlnativefc`

+

NOTE: Der `qlnativefc` Der Treiberparameter ist für die Adapter der Serie QLE 277x standardmäßig eingestellt. Sie müssen den folgenden Befehl ausführen, um ihn manuell für Adapter der Serie QLE 277x einzustellen.

+
[listing]
----
esxcfg-module -l | grep qlnativefc
qlnativefc               4    1912
----
. Überprüfen Sie, ob nvme auf dem Adapter aktiviert ist:
+
[listing]
----
  #esxcli storage core adapter list
HBA Name  Driver      Link State  UID                                   Capabilities         Description
--------  ----------  ----------  ------------------------------------  -------------------  -----------
 vmhba3    qlnativefc  link-up     fc.20000024ff1817ae:21000024ff1817ae  Second Level Lun ID  (0000:5e:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter    FC Adapter
vmhba4    qlnativefc  link-up     fc.20000024ff1817af:21000024ff1817af  Second Level Lun ID  (0000:5e:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter FC Adapter
vmhba64   qlnativefc  link-up     fc.20000024ff1817ae:21000024ff1817ae                       (0000:5e:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter  NVMe FC Adapter
vmhba65   qlnativefc  link-up     fc.20000024ff1817af:21000024ff1817af                       (0000:5e:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter  NVMe FC Adapter
----




== NVMe/FC validieren

. Vergewissern Sie sich, dass der NVMe/FC-Adapter auf dem ESXi Host aufgeführt ist:
+
[listing]
----
# esxcli nvme adapter list

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:qlnativefc:21000024ff1817ae  FC              qlnativefc
vmhba65  aqn:qlnativefc:21000024ff1817af  FC              qlnativefc
vmhba66  aqn:lpfc:100000109b579d9c 	      FC              lpfc
vmhba67  aqn:lpfc:100000109b579d9d 	      FC              lpfc

----
. Vergewissern Sie sich, dass die NVMe/FC-Namespaces ordnungsgemäß erstellt wurden:
+
Die UUIDs im folgenden Beispiel repräsentieren die NVMe/FC Namespace-Geräte.

+
[listing]
----
# esxcfg-mpath -b
uuid.5084e29a6bb24fbca5ba076eda8ecd7e : NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   vmhba65:C0:T0:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:69 WWPN: 21:00:34:80:0d:6d:72:69  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:2f:00:a0:98:df:e3:d1
   vmhba65:C0:T1:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:69 WWPN: 21:00:34:80:0d:6d:72:69  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:1a:00:a0:98:df:e3:d1
   vmhba64:C0:T0:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:68 WWPN: 21:00:34:80:0d:6d:72:68  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:18:00:a0:98:df:e3:d1
   vmhba64:C0:T1:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:68 WWPN: 21:00:34:80:0d:6d:72:68  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:19:00:a0:98:df:e3:d1
----
+

NOTE: In ONTAP 9.7 beträgt die Standardblockgröße für einen NVMe/FC Namespace 4 KB. Diese Standardgröße ist nicht mit ESXi kompatibel. Wenn Sie also Namespaces für ESXi erstellen, müssen Sie die Namespace-Blockgröße als 512 b festlegen. Sie können dies mit dem tun `vserver nvme namespace create` Befehl.

+
.Beispiel
`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

+
Siehe link:https://docs.netapp.com/ontap-9/index.jsp?topic=%2Fcom.netapp.doc.dot-cm-cmpr%2FGUID-5CB10C70-AC11-41C0-8C16-B4D0DF916E9B.html["ONTAP 9 Befehlsman-Pages"^] Entnehmen.

. Überprüfen Sie den Status der einzelnen ANA-Pfade der jeweiligen NVMe/FC-Namespace-Geräte:
+
[listing]
----
esxcli storage hpp path list -d uuid.5084e29a6bb24fbca5ba076eda8ecd7e
fc.200034800d6d7268:210034800d6d7268-fc.201700a098dfe3d1:201800a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba64:C0:T0:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active
   Path Config: {TPG_id=0,TPG_state=AO,RTP_id=0,health=UP}

fc.200034800d6d7269:210034800d6d7269-fc.201700a098dfe3d1:201a00a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba65:C0:T1:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active
   Path Config: {TPG_id=0,TPG_state=AO,RTP_id=0,health=UP}

fc.200034800d6d7269:210034800d6d7269-fc.201700a098dfe3d1:202f00a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba65:C0:T0:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active unoptimized
   Path Config: {TPG_id=0,TPG_state=ANO,RTP_id=0,health=UP}

fc.200034800d6d7268:210034800d6d7268-fc.201700a098dfe3d1:201900a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba64:C0:T1:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active unoptimized
   Path Config: {TPG_id=0,TPG_state=ANO,RTP_id=0,health=UP}
----




== Konfiguration von NVMe/TCP

Ab 7.0U3c werden die erforderlichen NVMe/TCP Module standardmäßig geladen. Informationen zur Konfiguration des Netzwerks und des NVMe/TCP-Adapters finden Sie in der Dokumentation zu VMware vSphere.



== NVMe/TCP validieren

.Schritte
. Überprüfen Sie den Status des NVMe/TCP-Adapters.
+
[listing]
----
[root@R650-8-45:~] esxcli nvme adapter list
Adapter    Adapter Qualified Name
--------- -------------------------------
vmhba64    aqn:nvmetcp:34-80-0d-30-ca-e0-T
vmhba65    aqn:nvmetc:34-80-13d-30-ca-e1-T
list
Transport Type   Driver   Associated Devices
---------------  -------  ------------------
TCP              nvmetcp    vmnzc2
TCP              nvmetcp    vmnzc3
----
. Verwenden Sie den folgenden Befehl, um die NVMe/TCP-Verbindungen aufzulisten:
+
[listing]
----
[root@R650-8-45:~] esxcli nvme controller list
Name
-----------
nqn.1992-08.com.netapp:sn.5e347cf68e0511ec9ec2d039ea13e6ed:subsystem.vs_name_tcp_ss#vmhba64#192.168.100.11:4420
nqn.1992-08.com.netapp:sn.5e347cf68e0511ec9ec2d039ea13e6ed:subsystem.vs_name_tcp_ss#vmhba64#192.168.101.11:4420
Controller Number  Adapter   Transport Type   IS Online
----------------- ---------  ---------------  ---------
1580              vmhba64    TCP              true
1588              vmhba65    TCP              true

----
. Verwenden Sie den folgenden Befehl, um die Anzahl der Pfade zu einem NVMe-Namespace aufzulisten:
+
[listing]
----
[root@R650-8-45:~] esxcli storage hpp path list -d uuid.400bf333abf74ab8b96dc18ffadc3f99
tcp.vmnic2:34:80:Od:30:ca:eo-tcp.unknown-uuid.400bf333abf74ab8b96dc18ffadc3f99
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.400bf333abf74ab8b96dc18ffadc3f99
   Device Display Name: NVMe TCP Disk (uuid.400bf333abf74ab8b96dc18ffadc3f99)
   Path State: active unoptimized
   Path config: {TPG_id=0,TPG_state=ANO,RTP_id=0,health=UP}

tcp.vmnic3:34:80:Od:30:ca:el-tcp.unknown-uuid.400bf333abf74ab8b96dc18ffadc3f99
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.400bf333abf74ab8b96dc18ffadc3f99
   Device Display Name: NVMe TCP Disk (uuid.400bf333abf74ab8b96dc18ffadc3f99)
   Path State: active
   Path config: {TPG_id=0,TPG_state=AO,RTP_id=0,health=UP}
----




== Bekanntes Problem

* ESXi 7.0 U3 (und höher) NVMe/FC-Unterstützung ist ab ONTAP 9.9.1 P3 verfügbar, da wichtige NVMe-Abbruchfixes (herausgegeben von ESXi 7.0 U3 und höher) ab ONTAP 9.9.1 P3 verfügbar sind. Weitere Informationen finden Sie im entsprechenden öffentlichen bericht zu burt unter https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654[] Entsprechende Details.




== Verwandte Links

link:https://docs.netapp.com/us-en/netapp-solutions/virtualization/vsphere_ontap_ontap_for_vsphere.html["TR-4597-VMware vSphere with ONTAP"^]
link:https://kb.vmware.com/s/article/2031038["Unterstützung von VMware vSphere 5.x, 6.x und 7.x mit NetApp MetroCluster (2031038)"^]
link:https://kb.vmware.com/s/article/83370["Unterstützung für VMware vSphere 6.x und 7.x mit NetApp SnapMirror® Business Continuity (SM-BC)"^]
