---
sidebar: sidebar 
permalink: nvme_esxi_8.html 
keywords: nvme, esxi, ontap, nvme/fc, hypervisor 
summary: 'Sie können NVMe over Fabrics (NVMe-of) auf Initiator-Hosts, die ESXi 8.x ausführen, und ONTAP als Ziel konfigurieren.' 
---
= NVMe-of Host-Konfiguration für ESXi 8.x mit ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Sie können NVMe over Fabrics (NVMe-of) auf Initiator-Hosts, die ESXi 8.x ausführen, und ONTAP als Ziel konfigurieren.



== Instandhaltung

* Ab ONTAP 9.10.1 wird das NVMe-/TCP-Protokoll für ONTAP unterstützt.
* Ab ONTAP 9.9.1 P3 wird das NVMe/FC-Protokoll für ESXi 8 und höher unterstützt.




== Funktionen

* ESXi Initiator-Hosts können sowohl NVMe/FC- als auch FCP-Datenverkehr über dieselben Adapter-Ports ausführen. Siehe link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] Für eine Liste der unterstützten FC-Adapter und Controller. Siehe link:https://mysupport.netapp.com/matrix/["NetApp Interoperabilitäts-Matrix-Tool"^] Erhalten Sie auf der aktuellen Liste der unterstützten Konfigurationen und Versionen.
* Für ESXi 8.0 und höhere Versionen ist HPP (High Performance Plug-in) das Standard-Plug-in für NVMe Geräte.




== Bekannte Einschränkungen

* RDM-Zuordnung wird nicht unterstützt.




== NVMe/FC aktivieren

NVMe/FC ist in vSphere Versionen standardmäßig aktiviert.

.Host-NQN überprüfen
Sie müssen die NQN-Zeichenfolge des ESXi-Hosts überprüfen und überprüfen, ob sie mit der NQN-Zeichenfolge des Hosts für das entsprechende Subsystem auf dem ONTAP-Array übereinstimmt.

[listing]
----
# esxcli nvme info get
----
Beispielausgabe:

[listing]
----
Host NQN: nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
[listing]
----
# vserver nvme subsystem host show -vserver nvme_fc
----
Beispielausgabe:

[listing]
----
Vserver Subsystem Host NQN
------- --------- ----------------------------------------------------------
nvme_fc nvme_ss  nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
Wenn die Host-NQN-Strings nicht übereinstimmen, sollten Sie den verwenden `vserver nvme subsystem host add` Befehl zum Aktualisieren der korrekten Host-NQN-Zeichenfolge auf dem entsprechenden ONTAP-NVMe-Subsystem.



== Konfigurieren Sie Broadcom/Emulex und Marvell/Qlogic

Der `lpfc` Treiber und die `qlnativefc` Für Treiber in vSphere 8.x ist die NVMe/FC-Funktion standardmäßig aktiviert.

Siehe link:https://mysupport.netapp.com/matrix/["NetApp Interoperabilitäts-Matrix-Tool"^] Um zu überprüfen, ob die Konfiguration mit dem Treiber oder der Firmware unterstützt wird.



== NVMe/FC validieren

Folgende Verfahren stehen zur Validierung von NVMe/FC zur Verfügung:

.Schritte
. Überprüfen Sie, ob der NVMe/FC-Adapter auf dem ESXi-Host aufgeführt ist:
+
[listing]
----
# esxcli nvme adapter list
----
+
Beispielausgabe:

+
[listing]
----

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:lpfc:100000109b579f11        FC              lpfc
vmhba65  aqn:lpfc:100000109b579f12        FC              lpfc
vmhba66  aqn:qlnativefc:2100f4e9d456e286  FC              qlnativefc
vmhba67  aqn:qlnativefc:2100f4e9d456e287  FC              qlnativefc
----
. Überprüfen Sie, ob die NVMe/FC-Namespaces ordnungsgemäß erstellt wurden:
+
Die UUIDs im folgenden Beispiel repräsentieren die NVMe/FC Namespace-Geräte.

+
[listing, subs="+quotes"]
----
# esxcfg-mpath -b
uuid.116cb7ed9e574a0faf35ac2ec115969d : NVMe Fibre Channel Disk (*uuid.116cb7ed9e574a0faf35ac2ec115969d*)
   vmhba64:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:05:d0:39:ea:3a:b2:1f
   vmhba64:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:07:d0:39:ea:3a:b2:1f
   vmhba65:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:08:d0:39:ea:3a:b2:1f
   vmhba65:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:06:d0:39:ea:3a:b2:1f
----
+
[NOTE]
====
In ONTAP 9.7 beträgt die Standardblockgröße für einen NVMe/FC-Namespace 4 KB. Diese Standardgröße ist nicht mit ESXi kompatibel. Daher müssen Sie beim Erstellen von Namespaces für ESXi die Namespace-Blockgröße auf *512B* setzen. Sie können dies mit dem tun `vserver nvme namespace create` Befehl.

Beispiel:

`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

Siehe link:https://docs.netapp.com/us-en/ontap/concepts/manual-pages.html["ONTAP 9 Befehlsman-Pages"^] Entnehmen.

====
. Überprüfen Sie den Status der einzelnen ANA-Pfade der jeweiligen NVMe/FC-Namespace-Geräte:
+
[listing, subs="+quotes"]
----
# esxcli storage hpp path list -d uuid.df960bebb5a74a3eaaa1ae55e6b3411d

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2005d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2008d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2006d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2007d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

----




== Konfiguration von NVMe/TCP

In ESXi 8.x werden die erforderlichen NVMe/TCP-Module standardmäßig geladen. Informationen zur Konfiguration des Netzwerks und des NVMe/TCP-Adapters finden Sie in der VMware vSphere-Dokumentation.



== NVMe/TCP validieren

Zur Validierung von NVMe/TCP gehen Sie wie folgt vor.

.Schritte
. Überprüfen Sie den Status des NVMe/TCP-Adapters:
+
[listing]
----
esxcli nvme adapter list
----
+
Beispielausgabe:

+
[listing]
----
Adapter  Adapter Qualified Name           Transport Type  Driver   Associated Devices
-------  -------------------------------  --------------  -------  ------------------
vmhba65  aqn:nvmetcp:ec-2a-72-0f-e2-30-T  TCP             nvmetcp  vmnic0
vmhba66  aqn:nvmetcp:34-80-0d-30-d1-a0-T  TCP             nvmetcp  vmnic2
vmhba67  aqn:nvmetcp:34-80-0d-30-d1-a1-T  TCP             nvmetcp  vmnic3
----
. Liste der NVMe/TCP-Verbindungen abrufen:
+
[listing]
----
esxcli nvme controller list
----
+
Beispielausgabe:

+
[listing]
----
Name                                                  Controller Number  Adapter  Transport Type  Is Online  Is VVOL
---------------------------------------------------------------------------------------------------------  -----------------  -------
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.166:8009  256  vmhba64  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.165:4420 258  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.168:4420 259  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.166:4420 260  vmhba64  TCP  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.165:8009  261  vmhba64  TCP                  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba65#192.168.100.155:8009  262  vmhba65  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.167:4420 264  vmhba64  TCP  true    false

----
. Liste der Pfade zu einem NVMe-Namespace abrufen:
+
[listing, subs="+quotes"]
----
esxcli storage hpp path list -d *uuid.f4f14337c3ad4a639edf0e21de8b88bf*
----
+
Beispielausgabe:

+
[listing, subs="+quotes"]
----
tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.165:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T0:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.168:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T3:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.166:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T2:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.167:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T1:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}
----




== Bekannte Probleme

Die NVMe-of Hostkonfiguration für ESXi 8.x mit ONTAP weist folgende bekannte Probleme auf:

[cols="10,30,30"]
|===
| NetApp Bug ID | Titel | Beschreibung 


| link:https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654["1420654"^] | ONTAP Node ist nicht betriebsbereit, wenn das NVMe/FC-Protokoll mit ONTAP Version 9.9.1 verwendet wird | ONTAP 9.9.1 unterstützt jetzt den NVMe Befehl „abort“. Wenn ONTAP den Befehl „Abbrechen“ erhält, um einen mit NVMe fusionierten Befehl abzubrechen, der auf seinen Partnerbefehl wartet, tritt eine ONTAP Node-Unterbrechung auf. Das Problem wird nur bei Hosts bemerkt, die abgesicherte NVMe Befehle (z. B. ESX) und Fibre Channel (FC) verwenden. 


| 1543660 | I/O-Fehler tritt auf, wenn bei Linux VMs mit vNVMe Adaptern lange alle Pfade ausfallen (APD)  a| 
Linux-VMs, die vSphere 8.x und höher ausführen und virtuelle NVMe-Adapter (vNVME) verwenden, stoßen auf einen I/O-Fehler, da der vNVMe-Wiederholungsvorgang standardmäßig deaktiviert ist. Um eine Unterbrechung bei Linux VMs zu vermeiden, auf denen ältere Kernel während einer Alle Pfade unten (APD) ausgeführt werden, oder eine hohe I/O-Last zu vermeiden, hat VMware eine abstimmbare „VSCSIDisableNvmeRetry“ eingeführt, um den vNVMe-Wiederholungsvorgang zu deaktivieren.

|===
.Verwandte Informationen
link:https://docs.netapp.com/us-en/netapp-solutions/virtualization/vsphere_ontap_ontap_for_vsphere.html["TR-4597-VMware vSphere with ONTAP"^]
link:https://kb.vmware.com/s/article/2031038["Unterstützung von VMware vSphere 5.x, 6.x und 7.x mit NetApp MetroCluster (2031038)"^]
link:https://kb.vmware.com/s/article/83370["Unterstützung von VMware vSphere 6.x und 7.x mit NetApp SnapMirror Active Sync"^]
