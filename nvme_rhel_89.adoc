---
sidebar: sidebar 
permalink: nvme_rhel_89.html 
keywords: nvme, linux, rhel, red hat, enterprise 
summary: Konfigurieren des NVMe-of-Hosts für RHEL 8.9 mit ONTAP 
---
= NVMe-of Hostkonfiguration für RHEL 8.9 mit ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
NVMe over Fabrics (NVMe-of), einschließlich NVMe over Fibre Channel (NVMe/FC) und andere Übertragungen werden mit Red hat Enterprise Linux (RHEL) 8.9 mit Asymmetric Namespace Access (ANA) unterstützt. In NVMe-of Umgebungen entspricht ANA ALUA Multipathing in iSCSI- und FC-Umgebungen und wird mit in-Kernel NVMe Multipath implementiert.

Folgende Unterstützung ist für die NVMe-of-Hostkonfiguration für RHEL 8.9 mit ONTAP verfügbar:

* Unterstützung für NVMe over TCP (NVMe/TCP) neben NVMe/FC Über das NetApp-Plug-in im nativen nvme-cli-Paket werden ONTAP-Details für NVMe/FC- und NVMe/TCP-Namespaces angezeigt.


Weitere Informationen zu unterstützten Konfigurationen finden Sie im link:https://mysupport.netapp.com/matrix/["NetApp Interoperabilitäts-Matrix-Tool"^].



== Bekannte Einschränkungen

* NVMe Multipath im Kernel ist bei RHEL 8.9 NVMe-of Hosts standardmäßig deaktiviert. Deshalb müssen Sie sie manuell aktivieren.
* Auf RHEL 8.9-Hosts ist NVMe/TCP eine Technologie-Vorschaufunktion aufgrund offener Probleme.
* Das Booten von SAN über das NVMe-of-Protokoll wird derzeit nicht unterstützt.




== Aktivieren Sie in-Kernel Multipath

Sie können das folgende Verfahren verwenden, um Multipath im Kernel zu aktivieren.

.Schritte
. Installieren Sie RHEL 8.9 auf dem Hostserver.
. Überprüfen Sie nach Abschluss der Installation, ob Sie den angegebenen RHEL 8.9-Kernel ausführen:
+
[listing]
----
# uname -r
----
+
*Beispielausgabe*

+
[listing]
----
4.18.0-513.5.1.el8_9.x86_64
----
. Installieren Sie das nvme-cli-Paket:
+
[listing]
----
rpm -qa|grep nvme-cli
----
+
*Beispielausgabe*

+
[listing]
----
nvme-cli-1.16-9.el8.x86_64
----
. In -Kernel NVMe Multipath aktivieren:
+
[listing]
----
# grubby --args=nvme_core.multipath=Y --update-kernel /boot/vmlinuz-4.18.0-513.5.1.el8_9.x86_64
----
. Überprüfen Sie auf dem Host die NQN-Zeichenfolge des Hosts bei `/etc/nvme/hostnqn`:
+
[listing]
----
# cat /etc/nvme/hostnqn
----
+
*Beispielausgabe*

+
[listing]
----
nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0032-3410-8035-b8c04f4c5132
----
. Überprüfen Sie das `hostnqn` Die Zeichenfolge entspricht der `hostnqn` String für das entsprechende Subsystem auf dem ONTAP-Array:
+
[listing]
----
::> vserver nvme subsystem host show -vserver vs_fcnvme_141
----
+
*Beispielausgabe*

+
[listing]
----
Vserver     Subsystem       Host NQN
----------- --------------- ----------------------------------------------------------
vs_nvme101 rhel_101_QLe2772    nqn.2014-08.org.nvmexpress: uuid:4c4c4544-0032-3410-8035-b8c04f4c5132
----
+

NOTE: Wenn die Host-NQN-Zeichenfolgen nicht übereinstimmen, können Sie die verwenden `vserver modify` Befehl zum Aktualisieren der NQN-Zeichenfolge des Hosts auf dem entsprechenden ONTAP-NVMe-Subsystem, um die NQN-Zeichenfolge des Hosts zu entsprechen `/etc/nvme/hostnqn` Auf dem Host.

. Starten Sie den Host neu.


[NOTE]
====
Wenn Sie beabsichtigen, gleichzeitig NVMe und SCSI auf demselben Host auszuführen, empfiehlt NetApp die Verwendung des NVMe Multipath im Kernel für ONTAP-Namespaces und des dm-Multipath für ONTAP-LUNs. Dies sollte die ONTAP-Namespaces von dm-multipath ausschließen und verhindern, dass dm-multipath diese Namespace-Devices beansprucht. Sie können dies tun, indem Sie die hinzufügen `enable_foreign` Einstellung auf `/etc/multipath.conf` Datei:

[listing]
----
# cat /etc/multipath.conf
defaults {
  enable_foreign  NONE
}
----
====


== Konfiguration von NVMe/FC

Sie können NVMe/FC für Broadcom/Emulex- oder Marvell/Qlogic-Adapter konfigurieren.

[role="tabbed-block"]
====
.Broadcom/Emulex
--
.Schritte
. Stellen Sie sicher, dass Sie das unterstützte Adaptermodell verwenden:
+
[listing]
----
# cat /sys/class/scsi_host/host*/modelname
----
+
*Beispielausgabe:*

+
[listing]
----
LPe32002-M2
LPe32002-M2
----
+
[listing]
----
# cat /sys/class/scsi_host/host*/modeldesc
----
+
*Beispielausgabe:*

+
[listing]
----
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
----
. Vergewissern Sie sich, dass Sie das empfohlene Broadcom verwenden `lpfc` Firmware und Inbox-Treiber:
+
[listing]
----
# cat /sys/class/scsi_host/host*/fwrev
14.2.539.16, sli-4:2:c
14.2.539.16, sli-4:2:c
----
+
[listing]
----
# cat /sys/module/lpfc/version
0:14.0.0.21
----
+
Die aktuelle Liste der unterstützten Adaptertreiber- und Firmware-Versionen finden Sie unter link:https://mysupport.netapp.com/matrix/["NetApp Interoperabilitäts-Matrix-Tool"^].

. Verifizieren Sie das `lpfc_enable_fc4_type` Ist auf festgelegt `3`:
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
3
----
. Vergewissern Sie sich, dass die Initiator-Ports ausgeführt werden und dass die Ziel-LIFs angezeigt werden:
+
[listing]
----
# cat /sys/class/fc_host/host*/port_name
0x10000090fae0ec88
0x10000090fae0ec89
----
+
[listing]
----
# cat /sys/class/fc_host/host*/port_state
Online
Online
----
+
[listing, subs="+quotes"]
----
# cat /sys/class/scsi_host/host*/nvme_info
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x10000090fae0ec88 WWNN x20000090fae0ec88 DID x0a1300 *ONLINE*
NVME RPORT       WWPN x2049d039ea36a105 WWNN x2048d039ea36a105 DID x0a0c0a *TARGET DISCSRVC ONLINE*
NVME Statistics
LS: Xmt 0000000024 Cmpl 0000000024 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 00000000000001aa Issue 00000000000001ab OutIO 0000000000000001
        abort 00000002 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000002 Err 00000003
NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x10000090fae0ec89 WWNN x20000090fae0ec89 DID x0a1200 *ONLINE*
NVME RPORT       WWPN x204ad039ea36a105 WWNN x2048d039ea36a105 DID x0a080a *TARGET DISCSRVC ONLINE*
NVME Statistics
LS: Xmt 0000000024 Cmpl 0000000024 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 00000000000001ac Issue 00000000000001ad OutIO 0000000000000001
        abort 00000002 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00000002 Err 00000003



----


--
.Marvell/QLogic FC Adapter für NVMe/FC
--
Der native Inbox qla2xxx-Treiber, der im RHEL 8.9 GA-Kernel enthalten ist, verfügt über die neuesten Upstream-Fixes. Diese Fehlerbehebungen sind für die Unterstützung von ONTAP unerlässlich.

.Schritte
. Vergewissern Sie sich, dass der unterstützte Adaptertreiber und die unterstützten Firmware-Versionen ausgeführt werden:
+
[listing]
----
# cat /sys/class/fc_host/host*/symbolic_name
----
+
*Beispielausgabe*

+
[listing]
----
QLE2742 FW: v9.10.11 DVR: v10.02.08.200-k
QLE2742 FW: v9.10.11 DVR: v10.02.08.200-k
----
. Verifizieren Sie das `ql2xnvmeenable` Ist festgelegt. Dadurch kann der Marvell Adapter als NVMe/FC-Initiator verwendet werden:
+
[listing]
----
# cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
1
----


--
====


=== 1 MB E/A aktivieren (optional)

ONTAP meldet eine MDTS (MAX Data-Übertragungsgröße) von 8 in den Identifizieren von Controller-Daten. Das bedeutet, dass die maximale E/A-Anforderungsgröße bis zu 1 MB betragen kann. Um I/O-Anforderungen von Größe 1 MB für einen Broadcom-NVMe/FC-Host auszustellen, müssen Sie den `lpfc` Wert des `lpfc_sg_seg_cnt` Parameters ab dem Standardwert 64 auf 256 erhöhen.

.Schritte
. Setzen Sie den `lpfc_sg_seg_cnt` Parameter auf 256:
+
[listing]
----
# cat /etc/modprobe.d/lpfc.conf
options lpfc lpfc_sg_seg_cnt=256
----
. Führen Sie einen `dracut -f` Befehl aus, und starten Sie den Host neu:
. Stellen Sie sicher, dass `lpfc_sg_seg_cnt` 256:
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
256
----



NOTE: Dies gilt nicht für Qlogic NVMe/FC-Hosts.



== Konfiguration von NVMe/TCP

NVMe/TCP verfügt nicht über eine automatische Verbindungsfunktion. Wenn also ein Pfad ausfällt und nicht innerhalb der standardmäßigen Time-Out-Frist von 10 Minuten wieder hergestellt wird, kann NVMe/TCP die Verbindung nicht automatisch wiederherstellen. Um ein Timeout zu verhindern, sollten Sie den Wiederholungszeitraum für Failover-Ereignisse auf mindestens 30 Minuten einstellen.

.Schritte
. Vergewissern Sie sich, dass der Initiator-Port die Daten der Erkennungsprotokollseite über die unterstützten NVMe/TCP-LIFs abrufen kann:
+
[listing]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
*Beispielausgabe:*

+
[listing]
----
# nvme discover -t tcp -w 192.168.111.79 -a 192.168.111.14 -l 1800

Discovery Log Number of Records 8, Generation counter 18
=====Discovery Log Entry 0======
trtype:  tcp
adrfam:  ipv4
subtype: unrecognized
treq:    not specified.
portid:  0
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.154a5833c78c11ecb069d039ea359e4b: discovery
traddr:  192.168.211.15
sectype: none
=====Discovery Log Entry 1======
trtype:  tcp
adrfam:  ipv4
subtype: unrecognized
treq:    not specified.
portid:  1
trsvcid: 8009
subnqn:  nqn.1992-08.com.netapp:sn.154a5833c78c11ecb069d039ea359e4b: discovery
traddr:  192.168.111.15
sectype: none ..........


----
. Vergewissern Sie sich, dass die anderen LIF-Kombinationen des NVMe/TCP-Initiators die Daten der Erkennungsprotokollseite erfolgreich abrufen können:
+
[listing]
----
nvme discover -t tcp -w host-traddr -a traddr
----
+
*Beispielausgabe:*

+
[listing]
----
# nvme	discover	-t   tcp    -w	192.168.111.79   -a	192.168.111.14
# nvme	discover	-t   tcp    -w	192.168.111.79   -a	192.168.111.15
# nvme	discover	-t   tcp    -w	192.168.211.79   -a	192.168.211.14
# nvme	discover	-t   tcp    -w	192.168.211.79   -a	192.168.211.15


----
. Führen Sie die aus `nvme connect-all` Sie können alle unterstützten NVMe/TCP Initiator-Ziel-LIFs über die Nodes hinweg befehligen und die Zeitüberschreitung für den Controller für mindestens 30 Minuten oder 1800 Sekunden festlegen:
+
[listing]
----
nvme connect-all -t tcp -w host-traddr -a traddr -l 1800
----
+
*Beispielausgabe:*

+
[listing]
----
# nvme	connect-all	-t	tcp	-w	192.168.111.79	-a	192.168.111.14	-l	1800
# nvme	connect-all	-t	tcp	-w	192.168.111.79	-a	192.168.111.15	-l	1800
# nvme	connect-all	-t	tcp	-w	192.168.211.79	-a	192.168.211.14	-l	1800
# nvme	connect-all	-t	tcp	-w	192.168.211.79	-a	192.168.211.15	-l	1800


----




== NVMe-of validieren

Zur Validierung von NVMe-of gehen Sie wie folgt vor.

.Schritte
. Vergewissern Sie sich, dass das in-Kernel NVMe Multipath aktiviert ist:
+
[listing]
----
# cat /sys/module/nvme_core/parameters/multipath
Y
----
. Vergewissern Sie sich, dass die entsprechenden NVMe-of Einstellungen (z. B. `model` Auf einstellen `NetApp ONTAP Controller` Und Lastverteilung `iopolicy` Auf einstellen `round-robin`) Für die jeweiligen ONTAP-Namespaces werden auf dem Host korrekt wiedergegeben:
+
[listing]
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/model
NetApp ONTAP Controller
NetApp ONTAP Controller
----
+
[listing]
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
round-robin
round-robin
----
. Überprüfen Sie, ob die Namespaces auf dem Host erstellt und richtig erkannt wurden:
+
[listing]
----
# nvme list
----
+
*Beispielausgabe:*

+
[listing]
----
Node         SN                   Model
---------------------------------------------------------
/dev/nvme0n1 81Gx7NSiKSQqAAAAAAAB	NetApp ONTAP Controller


Namespace Usage    Format             FW             Rev
-----------------------------------------------------------
1                 21.47 GB / 21.47 GB	4 KiB + 0 B   FFFFFFFF
----
. Überprüfen Sie, ob der Controller-Status jedes Pfads aktiv ist und den korrekten ANA-Status aufweist:
+
[role="tabbed-block"]
====
.NVMe/FC
--
[listing]
----
# nvme list-subsys /dev/nvme3n1
----
*Beispielausgabe:*

[listing, subs="+quotes"]
----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.8e501f8ebafa11ec9b99d039ea359e4b:subsystem.rhel_163_Qle2742
+- nvme0 *fc* traddr=nn-0x204dd039ea36a105:pn-0x2050d039ea36a105 host_traddr=nn-0x20000024ff7f4994:pn-0x21000024ff7f4994 *live non-optimized*
+- nvme1 *fc* traddr=nn-0x204dd039ea36a105:pn-0x2050d039ea36a105 host_traddr=nn-0x20000024ff7f4994:pn-0x21000024ff7f4994 *live non-optimized*
+- nvme2 *fc* traddr=nn-0x204dd039ea36a105:pn-0x204fd039ea36a105 host_traddr=nn-0x20000024ff7f4995:pn-0x21000024ff7f4995 *live optimized*
+- nvme3 *fc* traddr=nn-0x204dd039ea36a105:pn-0x204ed039ea36a105 host_traddr=nn-0x20000024ff7f4994:pn-0x21000024ff7f4994 *live optimized*

----
--
.NVMe/TCP
--
[listing]
----
# nvme list-subsys /dev/nvme0n1
----
*Beispielausgabe:*

[listing, subs="+quotes"]
----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.154a5833c78c11ecb069d039ea359e4b:subsystem.rhel_tcp_165\
+- nvme0 *tcp* traddr=192.168.111.15 trsvcid=4420 host_traddr=192.168.111.79 *live non-optimized*
+- nvme1 *tcp* traddr=192.168.111.14 trsvcid=4420 host_traddr=192.168.111.79 *live optimized*
+- nvme2 *tcp* traddr=192.168.211.15 trsvcid=4420 host_traddr=192.168.211.79 *live non-optimized*
+- nvme3 *tcp* traddr=192.168.211.14 trsvcid=4420 host_traddr=192.168.211.79 *live optimized*

----
--
====
. Vergewissern Sie sich, dass das NetApp Plug-in für jedes ONTAP Namespace-Gerät die richtigen Werte anzeigt:
+
[role="tabbed-block"]
====
.Spalte
--
[listing]
----
# nvme netapp ontapdevices -o column
----
*Beispielausgabe:*

[listing]
----
Device        Vserver   Namespace Path
----------------------- ------------------------------
/dev/nvme0n1 vs_tcp79           /vol/vol1/ns


NSID       UUID                                   Size
------------------------------------------------------------
1          aa197984-3f62-4a80-97de-e89436360cec	21.47GB
----
--
.JSON
--
[listing]
----
# nvme netapp ontapdevices -o json
----
*Beispielausgabe*

[listing]
----
{
  "ONTAPdevices”: [
    {
      "Device”: "/dev/nvme0n1",
      "Vserver”: "vs_tcp79",
      "Namespace Path”: "/vol/vol1/ns",
      "NSID”: 1,
      "UUID”: "aa197984-3f62-4a80-97de-e89436360cec",
      "Size”: "21.47GB",
      "LBA_Data_Size”: 4096,
      "Namespace Size" : 5242880
    },
]

}


----
--
====




== Bekannte Probleme

Die NVMe-of Host-Konfiguration für RHEL 8.9 mit ONTAP-Version weist das folgende bekannte Problem auf:

[cols="20,40,40"]
|===
| NetApp Bug ID | Titel | Beschreibung 


| link:https://mysupport.netapp.com/site/bugs-online/product/HOSTUTILITIES/BURT/1479047["1479047"^] | RHEL 8.9 NVMe-of-Hosts erstellen doppelte persistente Erkennungs-Controller | Auf NVMe over Fabrics-Hosts (NVMe-of) können Sie den Befehl „nvme discover -p“ verwenden, um persistente Discovery Controller (PDCs) zu erstellen. Wenn dieser Befehl verwendet wird, sollte pro Initiator-Zielkombination nur ein PDC erstellt werden.  Wenn Sie jedoch Red hat Enterprise Linux (RHEL) 8.9 auf einem NVMe-of-Host ausführen, wird bei jeder Ausführung von „nvme discover -p“ ein doppelter PDC erstellt. Dies führt zu einer unnötigen Nutzung der Ressourcen auf dem Host und dem Ziel. 
|===